{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20edd000",
   "metadata": {},
   "source": [
    "# CSC 380 Artificial Intelligence I (Fall 2025)\n",
    "# HW\\#2 FrozenLake Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1691a6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\ntomuro\\appdata\\roaming\\python\\python312\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ntomuro\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ntomuro\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\ntomuro\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\ntomuro\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "# First import gymnasium (https://gymnasium.farama.org/)\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981a755",
   "metadata": {},
   "source": [
    "## Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195ba04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "\n",
    "# Create a FrozenLake 8x8 environment using Gymnasium\n",
    "# (https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True, render_mode=\"ansi\")\n",
    "\n",
    "# This line is critically needed to access the state and the probabilities \n",
    "# in the environment due to the recent code update by Gymnasium.\n",
    "env_unwrapped = env.unwrapped\n",
    "\n",
    "# Reset the environment and display it (in ansi ascii)\n",
    "env.reset()\n",
    "print (env.render())  # wrap render() in print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd47c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively you can create a random map, as described in \n",
    "## the FrozenLake documentation page.  It's commented out for now.\n",
    "\n",
    "#from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "#env = gym.make('FrozenLake-v1', desc=generate_random_map(size=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6c2e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make one (random) action\n",
    "action = env.action_space.sample()\n",
    "print (action)\n",
    "env.step(action)\n",
    "print (env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29273b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print (env_unwrapped.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468102a6",
   "metadata": {},
   "source": [
    "## Inspect the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40d9fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states: 64\n",
      "number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "nS = int(env.observation_space.n)    # number of states -- 8x8=64\n",
    "nA = int(env.action_space.n)         # number of actions -- four directions; 0:left, 1:down, 2:right, 3:up\n",
    "print (f\"number of states: {nS}\\nnumber of actions: {nA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3d7a9",
   "metadata": {},
   "source": [
    "Note that actions are 0-based integers.  You can check in the Gymnasium source code (https://github.com/Farama-Foundation/Gymnasium/blob/d71a13588266256a4c900b5e0d72d10785816c3a/gymnasium/envs/toy_text/frozen_lake.py)\n",
    "\n",
    "- 0: Move left\n",
    "- 1: Move down\n",
    "- 2: Move right\n",
    "- 3: Move up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f87f5",
   "metadata": {},
   "source": [
    "### All of the environment's probabilities are stored in 'env_unwrapped.P'.  \n",
    "It is a dictionary, keyed by the state index e.g. env_unwrapped.P[0], env_unwrapped.P[1] etc.)\n",
    "\n",
    "Then for each state, the value is a dictionary, keyed by the actions (0-based).  Then for each action, the value is a list, showing the **probability of transitioning into the next state, the index of the next state, reward, and True/False** (done=True if the next state is a Hole or the Goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22296b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probabilies from State 0\n",
    "env_unwrapped.P[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd330b1",
   "metadata": {},
   "source": [
    "### Note on the environment (from the Gymnasium page)\n",
    "is_slippery=True: If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions.\n",
    "\n",
    "For example, if action is left and is_slippery is True, then:\n",
    "\n",
    "- P(move left)=1/3\n",
    "- P(move up)=1/3\n",
    "- P(move down)=1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ac62fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 54, 0.0, True),\n",
       "  (0.3333333333333333, 61, 0.0, False),\n",
       "  (0.3333333333333333, 62, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 61, 0.0, False),\n",
       "  (0.3333333333333333, 62, 0.0, False),\n",
       "  (0.3333333333333333, 63, 1.0, True)],\n",
       " 2: [(0.3333333333333333, 62, 0.0, False),\n",
       "  (0.3333333333333333, 63, 1.0, True),\n",
       "  (0.3333333333333333, 54, 0.0, True)],\n",
       " 3: [(0.3333333333333333, 63, 1.0, True),\n",
       "  (0.3333333333333333, 54, 0.0, True),\n",
       "  (0.3333333333333333, 61, 0.0, False)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probabilies from State 62 (left of the Goal state).  \n",
    "# Notice some 'True' results (implying the goal is reached).\n",
    "env_unwrapped.P[62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d84c72",
   "metadata": {},
   "source": [
    "# ** Functions to create a fixed deterministic policy, and to run one experiment for a given number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be000efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_policy(num_actions, num_states, seed=None):\n",
    "    \"\"\"\n",
    "    A policy is a 1D array of length # of states, where each element is a\n",
    "    number between 0 (inclusive) and # of actions (exclusive) randomly chosen.\n",
    "    If a specific seed is passed, the same numbers are genereated, while\n",
    "    if the seed is None, the numbers are unpredictable every time.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.integers(low=0, high=num_actions, size=num_states)\n",
    "\n",
    "\n",
    "def run_oneexperiment(env, policy, num_episodes, display=False):\n",
    "    \"\"\"\n",
    "    Run one experiment, when agent follows a policy, for a given number of episodes.\n",
    "    \"\"\"    \n",
    "    # Count the number of goals made and getting stuck in a hole\n",
    "    goals = 0\n",
    "    holes = 0\n",
    "    # Total rewards and steps\n",
    "    total_rewards = 0\n",
    "    total_goal_steps = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        # For each time,\n",
    "        env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        steps = 0\n",
    "\n",
    "        if display:\n",
    "            episode = [(env.env_unwrapped.s)] # initial state (in a tuple)\n",
    "\n",
    "        while not done:\n",
    "            # choose the action based on the policy\n",
    "            state = env.s\n",
    "            action = policy[state]\n",
    "\n",
    "            # take the action\n",
    "            next_state, reward, done, info, p = env.step(action)\n",
    "            steps += 1\n",
    "\n",
    "            # extend the episode\n",
    "            if display:\n",
    "                episode.append(tuple([action,next_state]))\n",
    "            # accumulate rewards\n",
    "            rewards += reward\n",
    "        \n",
    "        # Calculate stats\n",
    "        total_rewards += rewards\n",
    "        if reward == 1.0: # Goal, or env.s == 63\n",
    "            goals += 1\n",
    "            total_goal_steps += steps\n",
    "        else:\n",
    "            holes += 1\n",
    "            \n",
    "        # Display\n",
    "        if display:\n",
    "            print (env.render())\n",
    "            \n",
    "    # One experiment finished,\n",
    "    return goals, holes, total_rewards, total_goal_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19c360",
   "metadata": {},
   "source": [
    "### * A utility function to display a 1D array/policy in a 2D array/grid *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d420f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def display_policy(policy):\n",
    "    side = int(math.sqrt(nS))  # assuming a square\n",
    "    policy = policy.reshape((side, side))\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d6c4c",
   "metadata": {},
   "source": [
    "## One experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d6b8bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Policy ***\n",
      "[[2 3 0 0 1 2 3 1]\n",
      " [0 0 1 1 3 1 2 2]\n",
      " [0 2 0 0 2 1 1 2]\n",
      " [0 0 2 3 2 3 1 0]\n",
      " [2 2 2 2 1 0 2 0]\n",
      " [1 0 2 1 1 2 0 1]\n",
      " [2 3 0 3 1 1 3 0]\n",
      " [0 1 2 3 1 1 3 3]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TimeLimit' object has no attribute 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Policy ***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(display_policy(policy)))\n\u001b[0;32m      4\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m      6\u001b[0m goals, holes, total_rewards, total_goal_steps \\\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;241m=\u001b[39m run_oneexperiment(env, policy, num_episodes)\n\u001b[0;32m      9\u001b[0m percent_goal \u001b[38;5;241m=\u001b[39m goals \u001b[38;5;241m/\u001b[39m num_episodes\n\u001b[0;32m     10\u001b[0m percent_hole \u001b[38;5;241m=\u001b[39m holes \u001b[38;5;241m/\u001b[39m num_episodes\n",
      "Cell \u001b[1;32mIn[12], line 35\u001b[0m, in \u001b[0;36mrun_oneexperiment\u001b[1;34m(env, policy, num_episodes, display)\u001b[0m\n\u001b[0;32m     31\u001b[0m     episode \u001b[38;5;241m=\u001b[39m [(env\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39ms)] \u001b[38;5;66;03m# initial state (in a tuple)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# choose the action based on the policy\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39ms\n\u001b[0;32m     36\u001b[0m     action \u001b[38;5;241m=\u001b[39m policy[state]\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# take the action\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TimeLimit' object has no attribute 's'"
     ]
    }
   ],
   "source": [
    "policy = generate_random_policy(nA, nS, 17) # change seed to a specific number, or None (default)\n",
    "print (\"*** Policy ***\\n{}\".format(display_policy(policy)))\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "goals, holes, total_rewards, total_goal_steps \\\n",
    "    = run_oneexperiment(env, policy, num_episodes)\n",
    "\n",
    "percent_goal = goals / num_episodes\n",
    "percent_hole = holes / num_episodes\n",
    "mean_reward = total_rewards / num_episodes\n",
    "mean_goal_steps = 0.0 if (goals == 0) else (total_goal_steps / goals)\n",
    "\n",
    "print (\"\\n*** RESULTS ***:\\nGoals: {:>5d}/{} = {:>7.3%}\\nHoles: {:>5d}/{} = {:>7.3%}\"\n",
    "       .format(goals, num_episodes, percent_goal, holes, num_episodes, percent_hole))\n",
    "print(\"mean reward:          {:.5f}\\nmean goal steps:     {:.2f}\".format(mean_reward, mean_goal_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d9b03df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "3\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "0\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "0\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "1\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    print (action)\n",
    "    env.step(action)\n",
    "    print (env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98f9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
